<div class="publications">
  <h2>Publications</h2>
  <hr>
  
  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/fair_pricing.png" alt="Fair Pricing" width="300" height="200">
    </div>
    <div class="publication-content">
      <a href="https://arxiv.org/abs/2404.14620">
        <papertitle>Fairness Incentives in Response to Unfair Dynamic Pricing</papertitle>
      </a>
      <br>
      <strong>Jesse Thibodeau</strong>,
      <strong>Hadi Nekoei</strong>,
      <strong>Afaf Ta√Øk</strong>,
      <strong>Janarthanan Rajendran</strong>
      <strong>Golnoosh Farnadi</strong>
      <br>
      <em>2024 ESIF Economics and AI+ML Meeting</em>
      <p class="content">We designed a basic simulated economy, wherein we introduce a dynamic social planner (SP)
      to generate corporate taxation schedules geared to incentivizing firms towards adopting fair pricing behaviours.</p>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2404.14620">Paper</a> /
        <a href="">code</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/MTMARL.png" alt="MTMARL" width="300" height="200">
    </div>
    <div class="publication-content">
      <a href="https://arxiv.org/abs/2302.02792">
        <papertitle>Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning</papertitle>
      </a>
      <br>
      <strong>Hadi Nekoei</strong>,
      <strong>Akilesh Badrinaaraayanan</strong>,
      <strong>Amit Sinha</strong>,
      <strong>Mohammad Amini</strong>
      <strong>Janarthanan Rajendran</strong>,
      <strong>Aditya Mahajan</strong>,
      <strong>Sarath Chandar</strong>
      <br>
      <em>2nd Conference on Lifelong Learning Agents (CoLLAs 2023), Workshop on Gamification and Multi-Agent Solutions (ICLR 2022)</em>
      <p class="content">An attempt to reduce non-stationarity in cooperative multi-agent RL inspired
      by two-time scale stochastic approximation (Borkar 1997).</p>
      <div class="publication-links">
        <a href="https://www.gamificationmas.com/accepted-papers">Paper</a> /
        <a href="">code</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/DEUP.png" alt="DEUP" width="300" height="200">
    </div>
    <div class="publication-content">
      <a href="https://arxiv.org/abs/2007.03158">
        <papertitle>DEUP: Direct Epistemic Uncertainty Prediction</papertitle>
      </a>
      <br>
      <strong>Moksh Jain</strong>,
      <strong>Salem Lahlou</strong>,
      <strong>Hadi Nekoei</strong>,
      <strong>Victor Butoi</strong>
      <strong>Paul Bertin</strong>,
      <strong>Jarrid Rector-Brooks</strong>,
      <strong>Maksym Korablyov</strong>,
      <strong>Yoshua Bengio</strong>
      <br>
      <em>Transactions on Machine Learning Research (TMLR), 2023</em>
      <p class="content">While previous work on epistemic uncertainty focused on model variance,
      DEUP directly estimates the reducible generalization error and thus takes into
      account model bias due to finite capacity or regularization.</p>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2102.08501">Paper</a> /
        <a href="https://github.com/MJ10/DEUP">code</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/lifelong-hanabi.jpg" alt="Lifelong Hanabi" width="300" height="150">
    </div>
    <div class="publication-content">
      <a href="https://arxiv.org/abs/2103.03216">
        <papertitle>Continuous Coordination As a Realistic Scenario for Lifelong Learning</papertitle>
      </a>
      <br>
      <strong>Hadi Nekoei*</strong>,
      <strong>Akilesh Badrinaaraayanan*</strong>,
      <strong>Aaron Courville</strong>,
      <strong>Sarath Chandar</strong>
      <br>
      <em>ICML 2021, NERL Workshop (Spotlight) ICLR 2021</em>
      <p class="content">In this work, we proposed Lifelong Hanabi as a new challenging benchmark for lifelong RL.
      The non-stationarity in our benchmark was introduced through agents having
      different strategies. We also showed that an IQL
      agent continually trained in our setup can zero-shot coordinate effectively with unseen agents.</p>
      <div class="publication-links">
        <a href="https://arxiv.org/pdf/2103.03216.pdf">Paper</a> /
        <a href="https://github.com/chandar-lab/Lifelong-Hanabi">code</a> /
        <a href="https://papertalk.org/papertalks/32808">video</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/LoCA.png" alt="LoCA" width="300" height="200">
    </div>
    <div class="publication-content">
      <a href="https://arxiv.org/abs/2007.03158">
        <papertitle>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning</papertitle>
      </a>
      <br>
      <strong>Harm Van Seijen</strong>,
      <strong>Hadi Nekoei</strong>,
      <strong>Evan Racah</strong>,
      <strong>Sarath Chandar</strong>
      <br>
      <em>NeurIPS, 2020</em>
      <p class="content">The common single-task sample-efficiency metric for evaluating
      deep model-based RL algorithms conflates improvements due to model-based learning
      with various other aspects, such as representation
      learning, making it difficult to assess true progress on model-based RL. To address
      this, we introduce an experimental setup and a metric, Local Change Adaptation
      (LoCA) regret, to evaluate model-based behavior of RL
      methods, inspired by work from neuroscience on detecting model-based behavior in
      humans and animals.</p>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2007.03158">Paper</a> /
        <a href="https://github.com/chandar-lab/LoCA">code</a> /
        <a href="https://papertalk.org/papertalks/8666">video</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/pommerman.jpg" alt="Pommerman" width="300" height="250">
    </div>
    <div class="publication-content">
      <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">
        <papertitle>Algorithmic Analysis and Improvements in Multi-Agent Reinforcement Learning in Partially Observable Setting</papertitle>
      </a>
      <br>
      <strong>Hadi Nekoei</strong>,
      <strong>Ahmed Afify</strong>,
      <strong>Juan Carrillo</strong>,
      <strong>Sriram Ganapathi Subramanian</strong>
      <p class="content">
      We introduce Multi Strategy LOLA and demonstrate its superiority in comparison with state-of-the-art algorithms on the game of Pommerman.</p>
      <div class="publication-links">
        <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">Poster</a> /
        <a href="https://youtu.be/J4Yl1UB3rM0">Video</a>
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="static/img/stock_paper.png" alt="Stock Paper" width="300" height="200">
    </div>
    <div class="publication-content">
      <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">
        <papertitle>Artificial Counselor System For Stock Investment</papertitle>
      </a>
      <br>
      <strong>H. Nekoei</strong>,
      <strong>Benyamin Ghojogh</strong>,
      <strong>Ali Sahebpasand</strong>,
      <strong>Mark Crowley</strong>
      <br>
      <em>IAAI, 2019</em>
      <p class="content">In this paper, the stock future prices
      (technical features) are predicted using Support Vector Regression.
      Thereafter, the predicted prices are used to recommend which portions of the budget
      an investor should invest in. Two different methods are used
      for suggesting best portions, which are Markowitz portfolio theory and fuzzy investment counselor.</p>
      <div class="publication-links">
        <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">Paper</a>
      </div>
    </div>
  </div>

</div>
