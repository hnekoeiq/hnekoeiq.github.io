
<div class="row">

	<h2>Publications</h2>
 <hr>
<table style="width:100%;border:1px;border-spacing:1px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
	<tr>
        <td width="30%"><img src="static/img/Staged_Learning.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

        <td valign="top" width="60%">
          <a href="https://www.gamificationmas.com/accepted-papers">
                  <papertitle>Staged independent learning: Towards decentralized cooperative multi-agent Reinforcement Learning</papertitle>
          </a>
        <br>
        <b> Hadi Nekoei</b>,
        <b> Akilesh Badrinaaraayanan </b>,
        <b> Amit Sinha</b>,
        <b> Mohammad Amini</b>
        <b> Janarthanan Rajendran </b>,
        <b> Aditya Mahajan</b>,
        <b> Sarath Chandar</b>
		<br>
              <em>Workshop on Gamification and Multi-Agent Solutions (ICLR 2022)</em><br>
            <p class="content">We present a general implementation of staged multi-agent RL algorithms
		    based on Sequential Iterative Best Response (SIBR) and two-time scale stochastic approximation (borkar1997),
		    and show that our new methods which we call Staged Independent Proximal
		    Policy Optimization (SIPPO) and Staged Independent Q-learning (SIQL) outperform SOTA independent learning on almost
		    all the tasks in the epymarl (papoudakis2020) benchmark. This can be seen as a first step towards more
		    decentralized MARL methods based on SIBR and multi-time scale learning.</p>
		</td>

        <td>
		<a href="https://www.gamificationmas.com/accepted-papers">Paper</a> /
               <a href="">code</a>
              
        </td>
	</tr>
	
	
	<tr>
        <td
                width="30%"><img src="static/img/lifelong-hanabi.jpg" alt="3DSP" width="300" height="150" style="border-style: none">
        </td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2103.03216">
                  <papertitle>Continuous Coordination As a Realistic Scenario for Lifelong Learning</papertitle>
          </a>
        <br>
        <b> Hadi Nekoei*</b>,
        <b> Akilesh Badrinaaraayanan*</b>,
        <b> Aaron Courville</b>,
        <b> Sarath Chandar</b>,

        <br>
              <em>ICML 2021, NERL Workshop (Spotlight) ICLR 2021</em><br>
        <p class="content">In this work, we proposed Lifelong Hanabi as a new challenging benchmark for lifelong RL.
            The non-stationarity in our benchmark was introduced through agents having
            different strategies instead of synthetic modifications to the
            environment or agent, while cross-play score served as an
            easy metric to quantify the similarity between tasks. We
            analyzed the performance of some well-known Lifelong Learning algorithms on this benchmark. We also showed that an IQL
            agent continually trained in our setup can zero-shot coordinate effectively with unseen agents.</p>
        </td>

        <td>
        <a href="https://arxiv.org/pdf/2103.03216.pdf">Paper</a> /
               <a href="https://github.com/chandar-lab/Lifelong-Hanabi">code</a> /
                  <a href="https://papertalk.org/papertalks/32808">video</a>

        </td>
	</tr>
	
	<tr>
        <td width="30%"><img src="static/img/DEUP.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2007.03158">
                  <papertitle>DEUP: Direct Epistemic Uncertainty Prediction</papertitle>
          </a>
        <br>
        <b> Moksh Jain</b>,
        <b> Salem Lahlou </b>,
        <b> Hadi Nekoei</b>,
        <b>Victor Butoi</b>
        <b> Paul Bertin </b>,
        <b> Jarrid Rector-Brooks</b>,
        <b>Maksym Korablyov</b>,
        <b>Yoshua Bengio </b>
		<br>
              <em>Arxiv, 2021</em><br>
            <p class="content">While previous work on epistemic uncertainty focused on model variance,
                DEUP directly estimates the reducible generalization error and thus takes into
                account model bias due to finite capacity or regularization. Comparative experimental results
                against standard epistemic uncertainty predictors  confirm the usefulness of DEUP, either in
                estimating this reducible error or in downstream tasks like sequential model  optimization
                and reinforcement learning with exploration.</p>
		</td>

        <td>
		<a href="https://arxiv.org/abs/2102.08501">Paper</a> /
               <a href="https://github.com/MJ10/DEUP">code</a>
              
        </td>
	</tr>
	
	<tr>
            <td width="30%"><img src="static/img/LoCA.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

            <td valign="top" width="60%">
              <a href="https://arxiv.org/abs/2007.03158">
                      <papertitle>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning</papertitle>
              </a>
            <br>
            <b> Harm Van Seijen</b>,
			<b> Hadi Nekoei </b>, 
			<b> Evan Racah</b>, 
		    <b>Sarath Chandar</b> 
		<br>
              <em>NeurIPS, 2020</em><br>
                <p class="content">The common single-task sample-efficiency metric for evaluating
                    deep model-based RL algorithms conflates improvements due to model-based learning
                    with various other aspects, such as representation
                    learning, making it difficult to assess true progress on model-based RL. To address
                    this, we introduce an experimental setup and a metric, Local Change Adaptation
                    (LoCA) regret, to evaluate model-based behavior of RL
                    methods, inspired by work from neuroscience on detecting model-based behavior in
                    humans and animals. Our metric can identify model-based behavior, even if the method
                    uses a poor representation and provides insight in how close a methodâ€™s behavior is
                    from optimal model-based behavior. </p>
		</td>
              
              <td>
		<a href="https://arxiv.org/abs/2007.03158">Paper</a> /
               <a href="https://github.com/chandar-lab/LoCA">code</a> /
                  <a href="https://papertalk.org/papertalks/8666">video</a>
              
            </td>
	</tr>

	 <tr>
            <td width="30%"><img src="static/img/pommerman.jpg" alt="3DSP" width="300" height="250" style="border-style: none"></td>

            <td valign="top" width="60%">
              <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">
                      <papertitle>Algorithmic Analysis and Improvements in Multi-Agent Reinforcement Learning in Partially Observable Setting</papertitle>
              </a>
            <br>
            <b> Hadi Nekoei</b>,
			<b> Ahmed Afify </b>, 
			<b> Juan Carrillo</b>, 
		    <b>Sriram Ganapathi Subramanian</b>
            <p class="content">Pommerman is a challenging multi-agent testbed published recently.
                In this work, we implement state of the art multi-agent reinforcement learning algorithms
                to train our agents on how to play the game. We also introduce a new algorithm called
                Multi Strategy LOLA and demonstrate its superiority in comparison with state-of-the-art algorithms.</p>
            </td>

              
              <td> <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">Poster</a> /
              <a href="https://youtu.be/J4Yl1UB3rM0">Video</a>
            <br>
            <br>
            </td>
	</tr>

	<tr>

            <td width="30%"><img src="static/img/stock_paper.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>
            <td valign="top" width="60%">
			<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">
            <papertitle>Artificial Counselor System For Stock Investment</papertitle>
            </a>
            <br>
            <b> H. Nekoei </b>,
            <b>Benyamin Ghojogh</b>,
	    <b>Ali Sahebpasand</b>,
            <b>Mark Crowley</b>
            
            <br>
              <em>IAAI, 2019</em><br>
            <br>
                <p class="content">This paper proposes a novel trading system which plays the role of
                    an artificial counselor for stock investment. In this paper, the stock future prices
                    (technical features) are predicted using Support Vector Regression.
                    Thereafter, the predicted prices are used to recommend which portions of the budget
                    an investor should invest in different existing stocks to have an optimum expected
                    profit considering their level of risk tolerance. Two different methods are used
                    for suggesting best portions, which are Markowitz portfolio theory and fuzzy investment counselor.</p>
            </td>
             <td>   <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">Paper</a>

            </td>
	</tr>

    </tbody>

</table>

</div>
