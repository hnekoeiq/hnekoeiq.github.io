
<div class="row">

	<h2>Publications</h2>
 <hr>
<table style="width:100%;border:1px;border-spacing:1px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
	<tr>
        <td width="30%"><img src="static/img/fair_pricing.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2302.02792">
                  <papertitle>Fairness Incentives in Response to Unfair Dynamic Pricing</papertitle>
          </a>
        <br>
        <b> Jesse Thibodeau</b>,
        <b> Hadi Nekoei </b>,
        <b> Afaf Ta√Øk</b>,
        <b> Janarthanan Rajendran</b>
        <b> Golnoosh Farnadi </b>,
		<br>
              <em>2024 ESIF Economics and AI+ML Meeting</em><br>
            <p class="content">We designed a basic simulated economy, wherein we introduce a dynamic social planner (SP)
		    to generate corporate taxation schedules geared to incentivizing firms towards adopting fair pricing behaviours.</p>
		</td>

        <td>
		<a href="https://www.gamificationmas.com/accepted-papers">Paper</a> /
               <a href="">code</a>
              
        </td>
	</tr>
	
	<tr>
        <td width="30%"><img src="static/img/MTMARL.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2302.02792">
                  <papertitle>Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning</papertitle>
          </a>
        <br>
        <b> Hadi Nekoei</b>,
        <b> Akilesh Badrinaaraayanan </b>,
        <b> Amit Sinha</b>,
        <b> Mohammad Amini</b>
        <b> Janarthanan Rajendran </b>,
        <b> Aditya Mahajan</b>,
        <b> Sarath Chandar</b>
		<br>
              <em>Workshop on Gamification and Multi-Agent Solutions (ICLR 2022)</em><br>
            <p class="content">An attempt to reduce non-stationarity in cooperative multi-agent RL inspired
		    by two-time scale stochastic approximation (Borkar 1997).</p>
		</td>

        <td>
		<a href="https://www.gamificationmas.com/accepted-papers">Paper</a> /
               <a href="">code</a>
              
        </td>
	</tr>
	
	
	<tr>
        <td width="30%"><img src="static/img/DEUP.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2007.03158">
                  <papertitle>DEUP: Direct Epistemic Uncertainty Prediction</papertitle>
          </a>
        <br>
        <b> Moksh Jain</b>,
        <b> Salem Lahlou </b>,
        <b> Hadi Nekoei</b>,
        <b>Victor Butoi</b>
        <b> Paul Bertin </b>,
        <b> Jarrid Rector-Brooks</b>,
        <b>Maksym Korablyov</b>,
        <b>Yoshua Bengio </b>
		<br>
              <em>Transactions on Machine Learning Research (TMLR), 2023 </em><br>
            <p class="content">While previous work on epistemic uncertainty focused on model variance,
                DEUP directly estimates the reducible generalization error and thus takes into
                account model bias due to finite capacity or regularization.</p>
		</td>

        <td>
		<a href="https://arxiv.org/abs/2102.08501">Paper</a> /
               <a href="https://github.com/MJ10/DEUP">code</a>
              
        </td>
	</tr>
	
	<tr>
        <td
                width="30%"><img src="static/img/lifelong-hanabi.jpg" alt="3DSP" width="300" height="150" style="border-style: none">
        </td>

        <td valign="top" width="60%">
          <a href="https://arxiv.org/abs/2103.03216">
                  <papertitle>Continuous Coordination As a Realistic Scenario for Lifelong Learning</papertitle>
          </a>
        <br>
        <b> Hadi Nekoei*</b>,
        <b> Akilesh Badrinaaraayanan*</b>,
        <b> Aaron Courville</b>,
        <b> Sarath Chandar</b>,

        <br>
              <em>ICML 2021, NERL Workshop (Spotlight) ICLR 2021</em><br>
        <p class="content">In this work, we proposed Lifelong Hanabi as a new challenging benchmark for lifelong RL.
            The non-stationarity in our benchmark was introduced through agents having
            different strategies. We also showed that an IQL
            agent continually trained in our setup can zero-shot coordinate effectively with unseen agents.</p>
        </td>

        <td>
        <a href="https://arxiv.org/pdf/2103.03216.pdf">Paper</a> /
               <a href="https://github.com/chandar-lab/Lifelong-Hanabi">code</a> /
                  <a href="https://papertalk.org/papertalks/32808">video</a>

        </td>
	</tr>
	
	
	<tr>
            <td width="30%"><img src="static/img/LoCA.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>

            <td valign="top" width="60%">
              <a href="https://arxiv.org/abs/2007.03158">
                      <papertitle>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning</papertitle>
              </a>
            <br>
            <b> Harm Van Seijen</b>,
			<b> Hadi Nekoei </b>, 
			<b> Evan Racah</b>, 
		    <b>Sarath Chandar</b> 
		<br>
              <em>NeurIPS, 2020</em><br>
                <p class="content">The common single-task sample-efficiency metric for evaluating
                    deep model-based RL algorithms conflates improvements due to model-based learning
                    with various other aspects, such as representation
                    learning, making it difficult to assess true progress on model-based RL. To address
                    this, we introduce an experimental setup and a metric, Local Change Adaptation
                    (LoCA) regret, to evaluate model-based behavior of RL
                    methods, inspired by work from neuroscience on detecting model-based behavior in
                    humans and animals. </p>
		</td>
              
              <td>
		<a href="https://arxiv.org/abs/2007.03158">Paper</a> /
               <a href="https://github.com/chandar-lab/LoCA">code</a> /
                  <a href="https://papertalk.org/papertalks/8666">video</a>
              
            </td>
	</tr>

	 <tr>
            <td width="30%"><img src="static/img/pommerman.jpg" alt="3DSP" width="300" height="250" style="border-style: none"></td>

            <td valign="top" width="60%">
              <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">
                      <papertitle>Algorithmic Analysis and Improvements in Multi-Agent Reinforcement Learning in Partially Observable Setting</papertitle>
              </a>
            <br>
            <b> Hadi Nekoei</b>,
			<b> Ahmed Afify </b>, 
			<b> Juan Carrillo</b>, 
		    <b>Sriram Ganapathi Subramanian</b>
            <p class="content">
                We introduce Multi Strategy LOLA and demonstrate its superiority in comparison with state-of-the-art algorithms on the game of Pommerman.</p>
            </td>

              
              <td> <a href="https://1drv.ms/b/s!Aii679w1BZ6jbSjUl1ZranveN4s">Poster</a> /
              <a href="https://youtu.be/J4Yl1UB3rM0">Video</a>
            <br>
            <br>
            </td>
	</tr>

	<tr>

            <td width="30%"><img src="static/img/stock_paper.png" alt="3DSP" width="300" height="200" style="border-style: none"></td>
            <td valign="top" width="60%">
			<a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">
            <papertitle>Artificial Counselor System For Stock Investment</papertitle>
            </a>
            <br>
            <b> H. Nekoei </b>,
            <b>Benyamin Ghojogh</b>,
	    <b>Ali Sahebpasand</b>,
            <b>Mark Crowley</b>
            
            <br>
              <em>IAAI, 2019</em><br>
            <br>
                <p class="content">In this paper, the stock future prices
                    (technical features) are predicted using Support Vector Regression.
                    Thereafter, the predicted prices are used to recommend which portions of the budget
                    an investor should invest in. Two different methods are used
                    for suggesting best portions, which are Markowitz portfolio theory and fuzzy investment counselor.</p>
            </td>
             <td>   <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/5016">Paper</a>

            </td>
	</tr>

    </tbody>

</table>

</div>
